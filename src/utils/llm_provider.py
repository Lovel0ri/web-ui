from openai import OpenAI
import pdb
from langchain_openai import ChatOpenAI
from langchain_core.globals import get_llm_cache
from langchain_core.language_models.base import (
    BaseLanguageModel,
    LangSmithParams,
    LanguageModelInput,
)
import os
from langchain_core.load import dumpd, dumps
from langchain_core.messages import (
    AIMessage,
    SystemMessage,
    AnyMessage,
    BaseMessage,
    BaseMessageChunk,
    HumanMessage,
    convert_to_messages,
    message_chunk_to_message,
)
from langchain_core.outputs import (
    ChatGeneration,
    ChatGenerationChunk,
    ChatResult,
    LLMResult,
    RunInfo,
)
from langchain_ollama import ChatOllama
from langchain_core.output_parsers.base import OutputParserLike
from langchain_core.runnables import Runnable, RunnableConfig
from langchain_core.tools import BaseTool

from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Literal,
    Optional,
    Union,
    cast, List,
)
from langchain_anthropic import ChatAnthropic
from langchain_mistralai import ChatMistralAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_ollama import ChatOllama
from langchain_openai import AzureChatOpenAI, ChatOpenAI
from langchain_ibm import ChatWatsonx
from langchain_aws import ChatBedrock
from pydantic import SecretStr, Field

# Import requests and json forç›´æ¥è°ƒç”¨ Gemini API
import requests
import json

from src.utils import config
from src.utils import utils  # å‡è®¾ utils ä¸­æœ‰ encode_image ç­‰å‡½æ•°


class DeepSeekR1ChatOpenAI(ChatOpenAI):

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)
        self.client = OpenAI(
            base_url=kwargs.get("base_url"),
            api_key=kwargs.get("api_key")
        )

    async def ainvoke(
            self,
            input: LanguageModelInput,
            config: Optional[RunnableConfig] = None,
            *,
            stop: Optional[list[str]] = None,
            **kwargs: Any,
    ) -> AIMessage:
        message_history = []
        for input_ in input:
            if isinstance(input_, SystemMessage):
                message_history.append({"role": "system", "content": input_.content})
            elif isinstance(input_, AIMessage):
                message_history.append({"role": "assistant", "content": input_.content})
            else:
                message_history.append({"role": "user", "content": input_.content})

        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=message_history
        )

        reasoning_content = response.choices[0].message.reasoning_content
        content = response.choices[0].message.content
        return AIMessage(content=content, reasoning_content=reasoning_content)

    def invoke(
            self,
            input: LanguageModelInput,
            config: Optional[RunnableConfig] = None,
            *,
            stop: Optional[list[str]] = None,
            **kwargs: Any,
    ) -> AIMessage:
        message_history = []
        for input_ in input:
            if isinstance(input_, SystemMessage):
                message_history.append({"role": "system", "content": input_.content})
            elif isinstance(input_, AIMessage):
                message_history.append({"role": "assistant", "content": input_.content})
            else:
                message_history.append({"role": "user", "content": input_.content})

        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=message_history
        )

        reasoning_content = response.choices[0].message.reasoning_content
        content = response.choices[0].message.content
        return AIMessage(content=content, reasoning_content=reasoning_content)


class DeepSeekR1ChatOllama(ChatOllama):

    async def ainvoke(
            self,
            input: LanguageModelInput,
            config: Optional[RunnableConfig] = None,
            *,
            stop: Optional[list[str]] = None,
            **kwargs: Any,
    ) -> AIMessage:
        org_ai_message = await super().ainvoke(input=input)
        org_content = org_ai_message.content
        reasoning_content = org_content.split("</think>")[0].replace("<think>", "")
        content = org_content.split("</think>")[1]
        if "**JSON Response:**" in content:
            content = content.split("**JSON Response:**")[-1]
        return AIMessage(content=content, reasoning_content=reasoning_content)

    def invoke(
            self,
            input: LanguageModelInput,
            config: Optional[RunnableConfig] = None,
            *,
            stop: Optional[list[str]] = None,
            **kwargs: Any,
    ) -> AIMessage:
        org_ai_message = super().invoke(input=input)
        org_content = org_ai_message.content
        reasoning_content = org_content.split("</think>")[0].replace("<think>", "")
        content = org_content.split("</think>")[1]
        if "**JSON Response:**" in content:
            content = content.split("**JSON Response:**")[-1]
        return AIMessage(content=content, reasoning_content=reasoning_content)


class CustomChatGoogleGenerativeAI(ChatGoogleGenerativeAI):
    """
    Custom Gemini Chat æ¨¡å‹ï¼Œç”¨äºç›´æ¥è°ƒç”¨ Google Generative APIï¼ˆæ”¯æŒ text + imageï¼‰ã€‚
    ä¿®æ­£äº† URL é‡Œ model å‰ç¼€å¯èƒ½å‡ºç°çš„é‡å¤ â€œmodels/models/...â€ é—®é¢˜ã€‚
    """

    api_key: SecretStr = Field(default_factory=lambda: SecretStr(os.getenv("GOOGLE_API_KEY", "")))

    def __init__(self, *, model: str = "gemini-2.0-flash-exp", temperature: float = 0.0, api_key: Optional[str] = None, **kwargs: Any) -> None:
        """
        :param model: è¦è°ƒç”¨çš„ Gemini æ¨¡å‹åç§°ï¼ˆå¯ä»¥æ˜¯ "gemini-2.0-flash" æˆ–è€… "models/gemini-2.0-flash"ï¼‰ã€‚
        :param temperature: è°ƒç”¨æ—¶çš„ temperatureã€‚
        :param api_key: å¯é€‰ï¼Œå¦‚æœä¼ å…¥åˆ™è¦†ç›–ç¯å¢ƒå˜é‡ä¸­è¯»å–åˆ°çš„ api_keyã€‚
        """
        if api_key is not None:
            kwargs["api_key"] = SecretStr(api_key)
        super().__init__(model=model, temperature=temperature, **kwargs)
        # ChatGoogleGenerativeAI çˆ¶ç±»ä¼šæŠŠ model å­˜åˆ° self.modelï¼ŒæŠŠ temperature å­˜åˆ° self.temperatureï¼ŒæŠŠ api_key å­˜åˆ° self.api_key

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[Any] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """åŒæ­¥è°ƒç”¨ Gemini API å¹¶è¿”å› ChatResultã€‚"""

        # 1. å…ˆæŠŠ messages è½¬æˆ Gemini è¦æ±‚çš„ payload æ ¼å¼
        gemini_messages = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                parts = []
                if isinstance(msg.content, str):
                    parts.append({"text": msg.content})
                elif isinstance(msg.content, list):
                    for item in msg.content:
                        if isinstance(item, dict) and item.get("type") == "text":
                            parts.append({"text": item["text"]})
                        elif isinstance(item, dict) and item.get("type") == "image_url":
                            image_url = item["image_url"]["url"]
                            if image_url.startswith("data:image/"):
                                mime_type = image_url.split(";")[0].split(":")[1]
                                base64_data = image_url.split(",")[1]
                                parts.append({"inline_data": {"mime_type": mime_type, "data": base64_data}})
                gemini_messages.append({"role": "user", "parts": parts})
            elif isinstance(msg, AIMessage):
                gemini_messages.append({"role": "model", "parts": [{"text": msg.content}]})
            elif isinstance(msg, SystemMessage):
                # å¦‚æœå·²æœ‰ç”¨æˆ·æ¶ˆæ¯å°±æ’åˆ°ç¬¬ä¸€æ¡çš„ parts é‡Œï¼Œå¦åˆ™å½“æˆæ–°çš„ user æ¶ˆæ¯
                if gemini_messages and gemini_messages[0]["role"] == "user":
                    gemini_messages[0]["parts"].insert(0, {"text": msg.content + "\n"})
                else:
                    gemini_messages.insert(0, {"role": "user", "parts": [{"text": msg.content}]})

        # 2. ä¿®æ­£ model_name_for_urlï¼šå¦‚æœ self.model å·²ç»ä»¥ "models/" å¼€å¤´ï¼Œå°±ç›´æ¥ç”¨ï¼›å¦åˆ™æ‰‹åŠ¨æ‹¼ "models/{self.model}"
        if self.model.startswith("models/"):
            model_name_for_url = self.model
        else:
            model_name_for_url = f"models/{self.model}"

        # 3. æ„é€ æœ€ç»ˆçš„ URLï¼ˆç¡®è®¤åªæœ‰ä¸€ä¸ª "models/" å‰ç¼€ï¼‰
        api_url = (
            f"https://generativelanguage.googleapis.com/v1beta/{model_name_for_url}:generateContent"
            f"?key={self.api_key.get_secret_value()}"
        )

        payload = {
            "contents": gemini_messages,
            "generationConfig": {
                "temperature": self.temperature,
                "maxOutputTokens": kwargs.get("max_tokens", 2048),
            },
        }

        try:
            response = requests.post(
                api_url,
                headers={"Content-Type": "application/json"},
                json=payload
            )
            response.raise_for_status()
            result = response.json()

            text_content = result["candidates"][0]["content"]["parts"][0]["text"]
            chat_generation = ChatGeneration(message=AIMessage(content=text_content))
            return ChatResult(generations=[chat_generation])

        except Exception as e:
            raise ValueError(
                f"Error calling Gemini API: {e}\n"
                f"URL: {api_url}\n"
                f"Response: {response.text if 'response' in locals() else 'No response'}"
            )

    async def _agenerate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[Any] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """å¼‚æ­¥è°ƒç”¨ï¼Œå†…éƒ¨ç›´æ¥è°ƒç”¨åŒæ­¥æ–¹æ³•ã€‚"""
        return self._generate(messages, stop, run_manager, **kwargs)



def get_llm_model(provider: str, **kwargs):
    """
    æ ¹æ® provider åç§°è¿”å›å¯¹åº”çš„ LLM å®ä¾‹ã€‚
    :param provider: LLM æä¾›å•†ï¼Œæ¯”å¦‚ "google"ã€"openai"ã€"anthropic" ç­‰
    :param kwargs: åŒ…å« model_nameã€temperatureã€api_keyã€base_url ç­‰
    :return: å¯¹åº”çš„ Chat æ¨¡å‹å®ä¾‹
    """
    api_key = None  # ç”¨æ¥å­˜å‚¨æœ€ç»ˆç¡®è®¤çš„ api_key

    if provider not in ["ollama", "bedrock"]:
        env_var = f"{provider.upper()}_API_KEY"
        api_key = kwargs.get("api_key", "") or os.getenv(env_var, "")
        if not api_key:
            provider_display = config.PROVIDER_DISPLAY_NAMES.get(provider, provider.upper())
            error_msg = f"ğŸ’¥ {provider_display} API key not found! ğŸ”‘ è¯·è®¾ç½®ç¯å¢ƒå˜é‡ `{env_var}` æˆ–è€…åœ¨ UI ä¸­å¡«å†™å®ƒã€‚"
            raise ValueError(error_msg)

    if provider == "anthropic":
        base_url = kwargs.get("base_url", "https://api.anthropic.com")
        return ChatAnthropic(
            model=kwargs.get("model_name", "claude-3-5-sonnet-20241022"),
            temperature=kwargs.get("temperature", 0.0),
            base_url=base_url,
            api_key=api_key,
        )

    elif provider == 'mistral':
        base_url = kwargs.get("base_url", os.getenv("MISTRAL_ENDPOINT", "https://api.mistral.ai/v1"))
        return ChatMistralAI(
            model=kwargs.get("model_name", "mistral-large-latest"),
            temperature=kwargs.get("temperature", 0.0),
            base_url=base_url,
            api_key=api_key,
        )

    elif provider == "openai":
        base_url = kwargs.get("base_url", os.getenv("OPENAI_ENDPOINT", "https://api.openai.com/v1"))
        return ChatOpenAI(
            model=kwargs.get("model_name", "gpt-4o"),
            temperature=kwargs.get("temperature", 0.0),
            base_url=base_url,
            api_key=api_key,
        )

    elif provider == "grok":
        base_url = kwargs.get("base_url", os.getenv("GROK_ENDPOINT", "https://api.x.ai/v1"))
        return ChatOpenAI(
            model=kwargs.get("model_name", "grok-3"),
            temperature=kwargs.get("temperature", 0.0),
            base_url=base_url,
            api_key=api_key,
        )

    elif provider == "deepseek":
        base_url = kwargs.get("base_url", os.getenv("DEEPSEEK_ENDPOINT", ""))
        if kwargs.get("model_name", "deepseek-chat") == "deepseek-reasoner":
            return DeepSeekR1ChatOpenAI(
                model=kwargs.get("model_name", "deepseek-reasoner"),
                temperature=kwargs.get("temperature", 0.0),
                base_url=base_url,
                api_key=api_key,
            )
        else:
            return ChatOpenAI(
                model=kwargs.get("model_name", "deepseek-chat"),
                temperature=kwargs.get("temperature", 0.0),
                base_url=base_url,
                api_key=api_key,
            )

    elif provider == "google":
        # æ³¨æ„è¿™é‡Œä¼ å…¥çš„æ˜¯ modelï¼ˆè€Œä¸æ˜¯ model_nameï¼‰ï¼Œå› ä¸ºçˆ¶ç±» ChatGoogleGenerativeAI æ„é€ å‡½æ•°æ¥å—çš„æ˜¯ model å‚æ•°ã€‚
        return CustomChatGoogleGenerativeAI(
            model=kwargs.get("model_name", "gemini-2.0-flash-exp"),
            temperature=kwargs.get("temperature", 0.0),
            api_key=api_key,
        )

    elif provider == "ollama":
        base_url = kwargs.get("base_url", os.getenv("OLLAMA_ENDPOINT", "http://localhost:11434"))
        if "deepseek-r1" in kwargs.get("model_name", "qwen2.5:7b"):
            return DeepSeekR1ChatOllama(
                model=kwargs.get("model_name", "deepseek-r1:14b"),
                temperature=kwargs.get("temperature", 0.0),
                num_ctx=kwargs.get("num_ctx", 32000),
                base_url=base_url,
            )
        else:
            return ChatOllama(
                model=kwargs.get("model_name", "qwen2.5:7b"),
                temperature=kwargs.get("temperature", 0.0),
                num_ctx=kwargs.get("num_ctx", 32000),
                num_predict=kwargs.get("num_predict", 1024),
                base_url=base_url,
            )

    elif provider == "azure_openai":
        base_url = kwargs.get("base_url", os.getenv("AZURE_OPENAI_ENDPOINT", ""))
        api_version = kwargs.get("api_version", os.getenv("AZURE_OPENAI_API_VERSION", "2025-01-01-preview"))
        return AzureChatOpenAI(
            model=kwargs.get("model_name", "gpt-4o"),
            temperature=kwargs.get("temperature", 0.0),
            api_version=api_version,
            azure_endpoint=base_url,
            api_key=api_key,
        )

    elif provider == "alibaba":
        base_url = kwargs.get("base_url", os.getenv("ALIBABA_ENDPOINT", "https://dashscope.aliyuncs.com/compatible-mode/v1"))
        return ChatOpenAI(
            model=kwargs.get("model_name", "qwen-plus"),
            temperature=kwargs.get("temperature", 0.0),
            base_url=base_url,
            api_key=api_key,
        )

    elif provider == "ibm":
        parameters = {
            "temperature": kwargs.get("temperature", 0.0),
            "max_tokens": kwargs.get("num_ctx", 32000)
        }
        base_url = kwargs.get("base_url", os.getenv("IBM_ENDPOINT", "https://us-south.ml.cloud.ibm.com"))
        return ChatWatsonx(
            model_id=kwargs.get("model_name", "ibm/granite-vision-3.1-2b-preview"),
            url=base_url,
            project_id=os.getenv("IBM_PROJECT_ID"),
            apikey=os.getenv("IBM_API_KEY"),
            params=parameters
        )

    elif provider == "moonshot":
        return ChatOpenAI(
            model=kwargs.get("model_name", "moonshot-v1-32k-vision-preview"),
            temperature=kwargs.get("temperature", 0.0),
            base_url=os.getenv("MOONSHOT_ENDPOINT"),
            api_key=api_key,
        )

    elif provider == "unbound":
        return ChatOpenAI(
            model=kwargs.get("model_name", "gpt-4o-mini"),
            temperature=kwargs.get("temperature", 0.0),
            base_url=os.getenv("UNBOUND_ENDPOINT", "https://api.getunbound.ai"),
            api_key=api_key,
        )

    elif provider == "siliconflow":
        base_url = kwargs.get("base_url", os.getenv("SiliconFLOW_ENDPOINT", ""))
        return ChatOpenAI(
            api_key=api_key,
            base_url=base_url,
            model_name=kwargs.get("model_name", "Qwen/QwQ-32B"),
            temperature=kwargs.get("temperature", 0.0),
        )

    elif provider == "modelscope":
        base_url = kwargs.get("base_url", os.getenv("MODELSCOPE_ENDPOINT", ""))
        return ChatOpenAI(
            api_key=api_key,
            base_url=base_url,
            model_name=kwargs.get("model_name", "Qwen/QwQ-32B"),
            temperature=kwargs.get("temperature", 0.0),
        )

    else:
        raise ValueError(f"Unsupported provider: {provider}")
